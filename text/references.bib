% This file was created with Citavi 6.4.0.35

@book{BellotRubio.2017,
 author = {{Bellot Rubio}, Luis R. and {Ruiz Cobo}, B.},
 year = {2017},
 title = {Inversion of Stokes profiles with SIR},
 institution = {{Instituto de Astrofisica de Andalucia} and {Instituto de Astrofisica de Canarias}}
}


@book{Rutten.2015,
 author = {Rutten, Robert J.},
 year = {2015},
 title = {Introduction to astrophysical radiative transfer: Lecture notes},
 institution = {{Sterrekundig Instituut Utrecht}}
}


@book{Rutten.2003,
 author = {Rutten, Robert J.},
 year = {2003},
 title = {Radiative transfer in stellar atmospheres: Lecture notes},
 institution = {{Sterrekundig Instituut Utrecht}}
}


@book{Rutten.1993,
 author = {Rutten, Robert J.},
 year = {1993},
 title = {Solar spectrum formation: Lecture notes},
 institution = {{Sterrekundig Instituut Utrecht}}
}


@article{RuizCobo.1992,
 author = {{Ruiz Cobo}, B. and {del Toro Iniesta}, J. C.},
 year = {1992},
 title = {Inversion of Stokes profiles},
 pages = {375},
 volume = {398},
 issn = {0004-637X},
 journal = {The Astrophysical Journal},
 doi = {10.1086/171862}
}


@misc{Rezende.21.05.2015,
 abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
 author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
 date = {21.05.2015},
 title = {Variational Inference with Normalizing Flows},
 url = {http://arxiv.org/pdf/1505.05770v6},
 file = {http://arxiv.org/abs/1505.05770v6},
 file = {https://arxiv.org/pdf/1505.05770v6.pdf}
}


@book{Raschka.2022,
 abstract = {bThis book of the bestselling and widely acclaimed Python Machine Learning series is a comprehensive guide to machine and deep learning using PyTorch's simple to code framework/bh4Key Features/h4ulliLearn applied machine learning with a solid foundation in theory/liliClear, intuitive explanations take you deep into the theory and practice of Python machine learning/liliFully updated and expanded to cover PyTorch, transformers, XGBoost, graph neural networks, and best practices/li/ulh4Book Description/h4Machine Learning with PyTorch and Scikit-Learn is a comprehensive guide to machine learning and deep learning with PyTorch. It acts as both a step-by-step tutorial and a reference you'll keep coming back to as you build your machine learning systems. Packed with clear explanations, visualizations, and examples, the book covers all the essential machine learning techniques in depth. While some books teach you only to follow instructions, with this machine learning book, we teach the principles allowing you to build models and applications for yourself. Why PyTorch? PyTorch is the Pythonic way to learn machine learning, making it easier to learn and simpler to code with. This book explains the essential parts of PyTorch and how to create models using popular libraries, such as PyTorch Lightning and PyTorch Geometric. You will also learn about generative adversarial networks (GANs) for generating new data and training intelligent agents with reinforcement learning. Finally, this new edition is expanded to cover the latest trends in deep learning, including graph neural networks and large-scale transformers used for natural language processing (NLP). This PyTorch book is your companion to machine learning with Python, whether you're a Python developer new to machine learning or want to deepen your knowledge of the latest developments.h4What you will learn/h4ulliExplore frameworks, models, and techniques for machines to 'learn' from data/liliUse scikit-learn for machine learning and PyTorch for deep learning/liliTrain machine learning classifiers on images, text, and more/liliBuild and train neural networks, transformers, and boosting algorithms/liliDiscover best practices for evaluating and tuning models/liliPredict continuous target outcomes using regression analysis/liliDig deeper into textual and social media data using sentiment analysis/li/ulh4Who this book is for/h4If you have a good grasp of Python basics and want to start learning about machine learning and deep learning, then this is the book for you. This is an essential resource written for developers and data scientists who want to create practical machine learning and deep learning applications using scikit-learn and PyTorch.Before you get started with this book, you'll need a good understanding of calculus, as well as linear algebra.},
 author = {Raschka, Sebastian},
 year = {2022},
 title = {Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python},
 url = {https://www.wiso-net.de/document/PKEB__9781801816380770},
 address = {Birmingham},
 edition = {1},
 publisher = {{Packt Publishing Limited}},
 isbn = {9781801819312}
}


@article{Papamakarios.,
 abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
 author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
 title = {Normalizing Flows for Probabilistic Modeling and Inference},
 url = {http://arxiv.org/pdf/1912.02762v2},
 journal = {Journal of Machine Learning Research},
 file = {http://arxiv.org/abs/1912.02762v2},
 file = {https://arxiv.org/pdf/1912.02762v2.pdf}
}


@misc{Lippe.17.06.2020,
 abstract = {Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate \emph{Categorical Normalizing Flows}, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.},
 author = {Lippe, Phillip and Gavves, Efstratios},
 date = {17.06.2020},
 title = {Categorical Normalizing Flows via Continuous Transformations},
 url = {http://arxiv.org/pdf/2006.09790v3},
 file = {http://arxiv.org/abs/2006.09790v3},
 file = {https://arxiv.org/pdf/2006.09790v3.pdf}
}


@article{Kobyzev.2021,
 abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
 author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
 year = {2021},
 title = {Normalizing Flows: An Introduction and Review of Current Methods},
 url = {http://arxiv.org/pdf/1908.09257v4},
 pages = {3964--3979},
 volume = {43},
 number = {11},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 doi = {10.1109/TPAMI.2020.2992934},
 file = {https://arxiv.org/pdf/1908.09257v4.pdf},
 file = {http://arxiv.org/abs/1908.09257v4}
}


@book{Rybicki.2008,
 author = {Rybicki, George B. and Lightman, Alan P.},
 year = {2008},
 title = {Radiative Processes in Astrophysics},
 keywords = {Astrophysik;Strahlungsprozess},
 address = {Weinheim},
 edition = {1., Auflage, neue Ausg},
 publisher = {Wiley-VCH},
 isbn = {352761818X},
 file = {http://d-nb.info/1055831460},
 file = {https://d-nb.info/1055831460/34},
 file = {https://nbn-resolving.org/urn:nbn:de:101:1-2014081524485}
}


@misc{Kingma.09.07.2018,
 abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
 author = {Kingma, Diederik P. and Dhariwal, Prafulla},
 date = {09.07.2018},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 url = {http://arxiv.org/pdf/1807.03039v2},
 file = {https://arxiv.org/pdf/1807.03039v2.pdf},
 file = {http://arxiv.org/abs/1807.03039v2}
}


@book{Goodfellow.2016,
 abstract = {{\textquotedbl}Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors{\textquotedbl}--Publisher's description



Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.},
 author = {Goodfellow, Ian and Courville, Aaron and Bengio, Yoshua},
 year = {2016},
 title = {Deep learning},
 url = {https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=2565107},
 address = {Cambridge, Massachusetts},
 publisher = {{The MIT Press}},
 isbn = {9780262035613},
 series = {Adaptive computation and machine learning}
}


@misc{Durkan.10.06.2019,
 abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
 author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
 date = {10.06.2019},
 title = {Neural Spline Flows},
 url = {http://arxiv.org/pdf/1906.04032v2},
 file = {http://arxiv.org/abs/1906.04032v2},
 file = {https://arxiv.org/pdf/1906.04032v2.pdf}
}


@book{Dullemond.2017,
 author = {Dullemond, C. P.},
 year = {2017},
 title = {Radiative Transfer in Astrophysics: Theory, Numerical Methods and Applications - Lecture notes},
 institution = {{University of Heidelberg}}
}


@misc{Dinh.27.05.2016,
 abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
 author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
 date = {27.05.2016},
 title = {Density estimation using Real NVP},
 url = {http://arxiv.org/pdf/1605.08803v3},
 file = {http://arxiv.org/abs/1605.08803v3},
 file = {https://arxiv.org/pdf/1605.08803v3.pdf}
}


@article{DiazBaso.2022,
 author = {{D{\'i}az Baso}, C. J. and {Asensio Ramos}, A. and de {La Cruz Rodr{\'i}guez}, J.},
 year = {2022},
 title = {Bayesian Stokes inversion with normalizing flows},
 pages = {A165},
 volume = {659},
 issn = {0004-6361},
 journal = {Astronomy {\&} Astrophysics},
 doi = {10.1051/0004-6361/202142018}
}


@article{delToroIniesta.2016,
 author = {{del Toro Iniesta}, Jose Carlos and {Ruiz Cobo}, Basilio},
 year = {2016},
 title = {Inversion of the radiative transfer equation for polarized light},
 volume = {13},
 number = {1},
 issn = {2367-3648},
 journal = {Living Reviews in Solar Physics},
 doi = {10.1007/s41116-016-0005-2}
}


@book{Deisenroth.2020,
 author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
 year = {2020},
 title = {Mathematics for machine learning},
 address = {Cambridge and New York, NY and Port Melbourne and New Delhi and Singapore},
 publisher = {{Cambridge University Press}},
 isbn = {9781108455145},
 doi = {10.1017/9781108679930},
 file = {https://www.gbv.de/dms/tib-ub-hannover/1679469878.pdf}
}


@book{DeglInnocenti.2005,
 abstract = {Description of Polarized Radiation -- Angular Momentum and Racah Algebra -- Atomic Spectroscopy -- Quantization of the Electromagnetic Field (Non-Relativistic Theory) -- Interaction of Material Systems with Polarized Radiation (the Classical Approach) -- Interaction of Material Systems with Polarized Radiation (the Quantum Approach) -- Statistical Equilibrium Equations and Radiative Transfer Coefficients for Atomic Systems -- Radiative Transfer for Polarized Radiation -- Line Formation in a Magnetic Field -- Non-Equilibrium Atomic Physics -- Astrophysical Applications: Solar Magnetometry -- Astrophysical Applications: Radiation Anisotropy in Stellar Atmospheres -- Astrophysical Applications: the Outer Layers of Stellar Atmospheres -- Astrophysical Applications: Stellar Atmospheres.},
 author = {Degl'Innocenti, Egidio Landi},
 year = {2005},
 title = {Polarization in Spectral Lines},
 address = {Dordrecht},
 volume = {307},
 publisher = {{Springer Netherlands}},
 isbn = {1-4020-2415-0},
 series = {Springer eBook Collection Physics and Astronomy},
 doi = {10.1007/1-4020-2415-0},
 file = {https://swbplus.bsz-bw.de/bsz264337808cov.jpg}
}


@article{BondTaylor.2022,
 abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
 author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
 year = {2022},
 title = {Deep Generative Modelling: A Comparative Review of VAEs, GANs,  Normalizing Flows, Energy-Based and Autoregressive Models},
 url = {http://arxiv.org/pdf/2103.04922v4},
 pages = {7327--7347},
 volume = {44},
 number = {11},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 doi = {10.1109/TPAMI.2021.3116668},
 file = {http://arxiv.org/abs/2103.04922v4},
 file = {https://arxiv.org/pdf/2103.04922v4.pdf}
}


@misc{Hoogeboom.30.01.2020,
 abstract = {Media is generally stored digitally and is therefore discrete. Many successful deep distribution models in deep learning learn a density, i.e., the distribution of a continuous random variable. Na\{\textquotedbl}ive optimization on discrete data leads to arbitrarily high likelihoods, and instead, it has become standard practice to add noise to datapoints. In this paper, we present a general framework for dequantization that captures existing methods as a special case. We derive two new dequantization objectives: importance-weighted (iw) dequantization and R\'enyi dequantization. In addition, we introduce autoregressive dequantization (ARD) for more flexible dequantization distributions. Empirically we find that iw and R\'enyi dequantization considerably improve performance for uniform dequantization distributions. ARD achieves a negative log-likelihood of 3.06 bits per dimension on CIFAR10, which to the best of our knowledge is state-of-the-art among distribution models that do not require autoregressive inverses for sampling.},
 author = {Hoogeboom, Emiel and Cohen, Taco S. and Tomczak, Jakub M.},
 date = {30.01.2020},
 title = {Learning Discrete Distributions by Dequantization},
 url = {http://arxiv.org/pdf/2001.11235v1},
 file = {https://arxiv.org/pdf/2001.11235v1.pdf},
 file = {http://arxiv.org/abs/2001.11235v1}
}


@book{Stix.2002,
 abstract = {A wealth of new experimental and theoretical results has been obtained in solar physics since the first edition of this textbook appeared in 1989. Thus all nine chapters have been thoroughly revised, and about 100 pages and many new illustrations have been added to the text. The additions include element diffusion in the solar interior, the recent neutrino experiments, methods of image restoration, observational devices used for spectroscopy and polarimetry, and new developments in helioseismology and numerical simulation. The book takes particular advantage of the results of several recent space missions, which lead to substantial progress in our understanding of the Sun, from the deep interior to the corona and solar wind},
 author = {Stix, Michael},
 year = {2002},
 title = {The Sun: An Introduction},
 address = {Berlin and Heidelberg},
 edition = {Second Edition},
 publisher = {Springer},
 isbn = {978-3-642-62477-3},
 series = {Astronomy and Astrophysics Library},
 doi = {10.1007/978-3-642-56042-2}
}